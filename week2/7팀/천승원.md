[https://colab.research.google.com/drive/1Shif4v5W7wVLpPaTY3IHpljoFf4bqXnO#scrollTo=Qyq90ZzB2lnk](https://colab.research.google.com/drive/1Shif4v5W7wVLpPaTY3IHpljoFf4bqXnO#scrollTo=3BNtFKiZ2ln6)
# 데이터셋 정보 분석 내용
+ id : 날짜와 시간별 id
+ hour_bef_temperature : 1시간 전 기온
+ hour_bef_precipitation : 1시간 전 비 정보, 비가 오지 않았으면 0, 비가 오면 1
+ hour_bef_windspeed : 1시간 전 풍속(평균)
+ hour_bef_humidity : 1시간 전 습도
+ hour_bef_visibility : 1시간 전 시정(視程), 시계(視界)(특정 기상 상태에 따른 가시성을 의미)
+ hour_bef_ozone : 1시간 전 오존
+ hour_bef_pm10 : 1시간 전 미세먼지(머리카락 굵기의 1/5에서 1/7 크기의 미세먼지)
+ hour_bef_pm2.5 : 1시간 전 미세먼지(머리카락 굵기의 1/20에서 1/30 크기의 미세먼지)
+ count : 시간에 따른 따릉이 대여 수 
corr과 heatmap으로 상관계수 출력했을때, 결측값이 적으면서 상관관계가 높은 windspeed와 temprature를 사용한다.
count는 target, hour는 기본 분류값이다.

# 코드 흐름 + 사용한 모듈
---
기본적인 베이스라인은 동일하다. 사용된 모듈의 기능과 함수들은 대부분 코드내에 있으므로 중요한 것만 따로 뽑아서 정리했다.    
### 1. EDA(탐색적 자료분석)
```
head(), tail()/ shape/ info(), describe()      
```
는 순서대로 위에서부터 5개, 아래서부터 5개출력/ 데이터의 사이즈(행, 열) 출력 - ()안붙임/ 데이터의 형상(non_null갯수와 dtype), 기술통계량(평균, 최대최소, 개수, 중간값 등등) 출력
```
train.groupby('hour').mean()['count']
```
앞으로 계속 보게 될 최고로 중요한 부분.(EDA에선 지금까진 거의 이것만 알아도 될듯.)     
hour에 대해 묶고 그걸 평균값으로 정렬. 거기서 'count'열 뽑아냄.      
여기에 .plot()을 붙이기도 하고, plt.plot( - )로 그래프 만들어서 속성 변경하기도 하면서 시각화도 가능하다.     
그래프 관련 여러 속성들은 코드에 들어 있으니 생략.
```
import seaborn as sns
sns.heatmap(train.corr(), annot=True) #숫자 같이 보는 옵션
```
상관계수를 출력하는 코드. 여기서 모델링할때 사용할 변수를 정한다. 
### 2. 데이터 전처리
결측값을 없애주는 부분이다.
```
train[train['hour_bef_temperature'].isna()] 
```
1. 결측값 위치를 확인한 후
```
train.groupby('hour').mean()['hour_bef_temperature']
```
2. 시간별 평균값을 구해서
```
train['hour_bef_temprature'].fillna({인덱스:채워넣을값}, inplace=True)
```
3. fillna(inplace=True)에서 딕셔너리로 결측값 채워주면 된다.  
이 과정을 모델링에 사용하는 변수들 중 결측값 있는 것들(temp, windspeed)에 대해 반복.      
train 뿐만 아니라 test데이터도 마찬가지로 해준다.
### 3. 모델링
```
from sklearn.ensemble import RandomForestRegressor 
```
여기서 부터 해메지 말고 외워두자. RandomForest 불러오는 코드.
```
features = ['hour', 'hour_bef_temperature', 'hour_bef_windspeed']
X_train = train[features]
y_train = train['count']
X_test = train[features]
```
train 데이터들 중 모델링에 사용할 부분만 따로 분리하는 과정. 사용할 컬럼들만 features에 넣고 X_train과 같이 분리해낸다.   
y_train을 분리할때 count열을 사용하는데, 이때는 ''붙이는거 잊지 말자.   
```
model = RandomForestRegressor(n_estimators=100, random_state=0, max_depth=5)
model.fit(X_train, y_train)
pred = model.predict(X_test)
```
각종 파라미터를 이용해서 모델을 구성하고 그대로 학습, 예측값을 저장하는 부분이다.
```
submission['count'] = pred
submission.to_csv('model.csv', index=False)
```
파일로 저장하고 출력하는 부분. 저번과 동일하다. 

# 랜덤 포레스트
훈련을 통해 구성해놓은 나무들로부터 분류결과를 취합해서 결론을 얻는 학습과정.     
몇몇나무들이 과적합이 될 수 있지만, 다수의 나무를 기반으로 예측하기에 그 영향력이 줄어들어 좋은 일반화 성능을 보인다.      
+ 이렇게 좋은 성능을 얻기 위해 다수의 학습 알고리즘을 사용하는 것을 앙상블이라고 한다.     

모든 의사결정 트리는 학습 데이터 세트에서 임의로 하위 데이터 세트를 추출하여 생성된다. (ex - 1000개 중에 100개씩)
중복을 허용하기 때문에 단일 데이터가 여러번 선택될 수도 있다. 이 과정을 배깅(bagging)이라고 한다.
여기서 트리를 만들때, 사용될 속성을 제한함으로써 각 나무에 다양성을 준다.      
원래는 트리를 만들 때 모든 속성들을 살펴보고 정보 획득량이 가장 많은 속성을 선택해서 그걸 기준으로 데이터를 분할했지만, 이제는 각 분할에서 전체 속성들 중 일부만 고려하여 트리를 작성한다.     
참고로, 뽑는 속성의 개수는 전체 속성 개수의 제곱근만큼 선택하는 게 경험적으로 가장 좋다고 한다.

* 트리는 잘 모르겠다.    
https://hleecaster.com/ml-random-forest-concept/

