# Kaggle 데이터 분석 입문

## 1강 - What is Kaggle?

competition, kernels, discussion 세 개로 나눌 수 있음

**kaggle**은 2010년 설립된 빅데이터 솔루션 대회 플랫폼 회사

-2017년 3월에 구글에 인수

-현재 200만명의 회원

-data science, ML, DL을 주제로 모인 community

1) competition - data race for 데이터 과학자

기업, 정부기관, 단체, 연구소, 개인이 가진 dataset with prize → kaggle(개발환경, 커뮤니티) → 전 세계 데이터 사이언티스

-캐글에서 제공하는 가상환경(컴퓨터 수십, 수백 대 제공, with gpu)

-검증된 캐글러들이 자신이 분석한 것을 공유(좋은 reference, 공부자료)

## 2강 - Why do Kaggle?

문제 유형은 대부분 지도학습

캐글을 하는 이유 - 실력향상(ML certificate, Portfolio, 경험)

캐글 자체가 job을 주지 않는다. 하지만 캐글을 해서 ML, DL, DS 실력을 엄청 쌓아서, 그것이 job을 얻게한다. 

머신러닝, 데이터 분석 등의 객관적인 지표가 될 수 있음

- Exploratory data analysis
    - Data visualization
        - Matplotlib, Seaborn, Plotly
        - Data mining
        - Pandas, Numpy
- Data preparation
    - Data augmentation(imbalance)
        - Upsampling
        - Downsampling
        - SMOTE
- Feature engineering
    - Time series features
    - Categorical features
    - Numerical features
    - Aggregation features
    - Ratio features
    - Product features
    
- Model development
    - Sklearn
        - Linear model
        - Non-linear model
        - Tree-model
    - Not sklearn
        - Xgboost
        - Lightgbm
        - Catboost
        - LibFFM
    

데이터를 분석하려면 그림을 그려서 살펴봐야함 여러방식으로

데이터 불균형도 해소해줘야함 

<파이썬 머신러닝 완벽가이드>책 추천

딥러닝 

- Model evaluation
    - Various metrics
        - Accuracy
        - Precision
        - Recall
        - F1-score
        - Etc.
- Other technique
    - Machine learning pipeline
        - My pipeline code
    - Feature management

competition에 들어가서 열심히 올라가는 것이 중요하다!

## 3강 - Kaggle Competition 참여 방법

- 무엇부터 해야할까요?
    - **데이터 분석**/알고리즘선택
- 머신러닝?
    - Pattern recognition
    - Make general function(conditions) to obtain goal(minimize goal)
    - Learn statistics(correlations) between feature vs feature/feature vs target
    - Pattern, correlation이 있어서 AI가 찾았다!
    - →**Data**가 가장 중요함

Garbage in, Garbage out!

model을 바꾸는 것보다 **input을 다시 확인하자**

competition은 팀으로 참여하기를 권장(기업도 참여함)

1. **Data check** : **Null data check** → **Outlier check** 
2. **Feature engineering** : **Ratio features**(A per B) → **Product features**(A*B) → **Addition or Subtraction features**(A+B, A-B) → **New categorical features** → **Aggregation features**(Category와 numerical feature의 조합으로 생성// mean, median, variance, standard, deviation을 feature로 사용) → **Categorical features**(One-hot encoding, Label encoding, Lightgbm Built- in) → **Fill missing values and infinite values**(Numerical features, Categorical features) 
3. **Feature selection** : **Use various approaches**(feature 선택) 
4. **Model development** : **Use LGBM(**기승전 LGBM부터 쓰자 (빠르고 성능 굿))
5. **Traning strategy** : **Ensemble is always answer** (Sum of Week learner is stronger than One stronger learner. **stacking**하자 한번에 잘하려고 X)
6. **Stacking and Averaging** : 다른 팀과 합치기

**성능을 더 올리려면?** 

1. feature generation
2. parameter tuning
3. more stacking

다 해야합니다 순서는 1→3→(2)

Lesson1 : Feature generation과 Cross-validation은 함께 해야한다

validation set을 많이 만들고 feature을 만들고 leaderboard score과 비교

leaderboard submission 신중하게 하자

## 4강 - Kaggle 사용법

competition - titanic

회원가입 후 대회 선택

규칙 보기 

데이터 보고 파악해보기

데이터 분석할 때 그림을 그리기

그림으로 무엇을 얻을 것인가 생각하기

EDA to Prediction(Die Titanic)

Kernel 마스터,,,

novice

contributor 

expert

master
