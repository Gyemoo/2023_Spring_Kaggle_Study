# 1강 | What is Kaggle?
### Competition
기업, 정부기관, 단체, 연구소, 개인 등이 Kaggle에 데이터셋과 상금을 주면, Kaggle은 Dataset & 상금 & 개발환경(Kernel) & 커뮤니티 등을 통합하여 전 세계 데이터 사이언티스트들에게 제공한다. 
### 노트북
- 캐글에서 제공하는 가상환경(컴퓨터)
  - GPU는 2시간 제한
- 검증된 캐글러들이 자신의 분석 공유함
  - 좋은 reference이고 훌륭한 공부 자료다.
  
# 2강 | Why do Kaggle?
### 캐글을 해야하는 이유
- 컴피티션으로 머신러닝을 할 수 있는 대부분의 문제 유형을 담고 있다.
- 개인적 측면
  - 실력
    - ML certificate
    - Portfolio
    - 경험
  - 캐글 프로필 관리
  - 경험과 실력 쌓기 for 정형 데이터
    - 표준화된 형식이고, 구조가 잘 정의되어 있으며, 데이터 모델을 준수하고, 지속적인 순서를 따르고, 인간과 프로그램이 쉽게 액세스할 수 있는 데이터
  - 경험과 실력 for 딥러닝
    - 잘해서 컴피티션 하려 하지 말고, 컴피티션 해서 잘하자.
    
# 3강 | 캐글 컴피티션 참여 방법
### 머신 러닝은?
- Pattern recognition
- Make general function(conditions)
- to obtain goal(minimize loss)
- Learn statistics(correlations) between feature vs feature or feature vs target
### 데이터가 가장 중요하다
"GARBAGE IN, GARBAGE OUT!"
"당신의 모델은, 당신이 input으로 준 것만큼 좋다!"
- 성능 안 나오면 모델 보다 input 즉, 데이터를 손봐라
### 노트북 작성 시, 진행 순서
1. Data Check
  - NULL data check
  - Outlier check
2. Feature engineering
  - Ratio feature: A per B
  - Product feature: A x B
    - Feature importance 봤을 때, 상위 feature들 중 numerical feature들끼리 곱하여 추가함
  - Addtion feature: A + B
  - Subtraction feature: A - B
    - 중요한 feature끼리 더하거나 빼서 새로운 feature 생성
  - New categorical feature
    - 특정 기준을 만족하느냐, 만족하지 않느냐로 binary catgory 만들 수 있음
  - Aggregation feature
    - category와 numerical feature의 조합으로 생성
    - category 각 그룹 당 mean, median, variance, standard deviation을 feature로 사용
      - 조합하는 대로 다양하게 만들 수 있음
  - Categorical feature
    - one-hot encoding
      - Category 개수 만큼 column이 늘어난다.
      - 너무 오래 걸린다.
    - label encoding
      - 자칫 bias ordering 문제가 생길 수 있다
    - lightgbm Built-in
      - 데이터셋이 크니 학습이 빠른 lightgbm을 쓸건데, 카테고리를 처리하는 내장 알고리즘
  - Fill missing values and infinite values
    - Numerical features
    - Categorical fatures
3. Feature selection 
  - Use various approaches
    - correlation으로 거르기
4. Model development 
  - Use LGBM(넣으면 해결됨)
    - 빠르고 성능 좋음
5. Training strategy
  - Ensemble
    - 앙상블은 답이다. 강력하다. 
    - Sum of week learner is stronger than one strong learner.
### 위의 단계를 마친 후 발전시킬 수 있는 선택지
1. Feature generation
2. More stacking(앙상블 모델 늘리기)
3. Parameter tuning
### TIP
1. Feature generation과 Cross-validation은 함께 해야한다.
  - feature generation -> Cross validation system -> cv 성능 향상?
    - yes -> FeatureSet update
    - no -> feature generation 다시
2. Validation system을 잘 구축해야 한다.
  - 잘맞는 validation set을 보유해야 한다.
    - submit 점수로 validation과 test셋이 잘맞는가 추측
# 4강 | 캐글 사용법
- overview와 data 탭 잘 살펴보기
- 노트북 중 좋아요 가장 많이 받은 것부터 차례로 보기
  - 캐글은 따라 적기!
- 퍼블릭 노트북은 오픈소스
